"""
File: knowledge_graph.py
Purpose: Create knowledge graph from code analysis with enhanced semantic understanding
Generated by Code Reverse-Engineering System
"""

import logging
import networkx as nx
import json
from typing import Dict, List, Any, Optional, Tuple, Set
import os
from models import CodebaseAnalysis, CodeEntity
import ast
from dataclasses import dataclass
from enum import Enum

# Setup logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class SemanticRelationType(Enum):
    CALLS = "calls"
    IMPLEMENTS = "implements"
    EXTENDS = "extends"
    USES = "uses"
    CREATES = "creates"
    MODIFIES = "modifies"
    READS = "reads"
    WRITES = "writes"

@dataclass
class SemanticRelation:
    source: str
    target: str
    relation_type: SemanticRelationType
    context: Dict[str, Any]
    confidence: float

def create_knowledge_graph(analysis: CodebaseAnalysis) -> Dict[str, Any]:
    """
    Create a knowledge graph from codebase analysis results with enhanced semantic understanding.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        
    Returns:
        Dictionary with knowledge graph data, compatible with visualization tools.
    """
    logger.info("Creating enhanced knowledge graph from codebase analysis")
    
    # Create a directed graph
    graph = nx.DiGraph()
    
    # Add entities as nodes with enhanced metadata
    for entity in analysis.entities:
        # Enhanced node attributes
        node_attrs = {
            'type': entity.type,
            'path': entity.path,
            'code_size': len(entity.code.split('\n')),
            'metadata': entity.metadata,
            'semantic_context': extract_semantic_context(entity),
            'behavior_patterns': analyze_code_behavior(entity),
            'complexity_metrics': calculate_complexity_metrics(entity)
        }
        
        # Add node
        graph.add_node(entity.name, **node_attrs)
    
    # Add semantic relationships as edges
    semantic_relations = extract_semantic_relations(analysis)
    for relation in semantic_relations:
        graph.add_edge(
            relation.source,
            relation.target,
            relation_type=relation.relation_type.value,
            context=relation.context,
            confidence=relation.confidence
        )
    
    return convert_graph_to_visualization_format(graph)

def extract_semantic_context(entity: CodeEntity) -> Dict[str, Any]:
    """Extract semantic context from code entity."""
    try:
        tree = ast.parse(entity.code)
        context = {
            'imports': [],
            'classes': [],
            'functions': [],
            'variables': [],
            'control_flow': []
        }
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                context['imports'].extend(n.name for n in node.names)
            elif isinstance(node, ast.ImportFrom):
                context['imports'].append(f"{node.module}.{node.names[0].name}")
            elif isinstance(node, ast.ClassDef):
                context['classes'].append({
                    'name': node.name,
                    'bases': [base.id for base in node.bases if isinstance(base, ast.Name)],
                    'methods': [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                })
            elif isinstance(node, ast.FunctionDef):
                context['functions'].append({
                    'name': node.name,
                    'args': [arg.arg for arg in node.args.args],
                    'returns': get_return_type(node)
                })
        
        return context
    except Exception as e:
        logger.error(f"Error extracting semantic context: {e}")
        return {}

def analyze_code_behavior(entity: CodeEntity) -> Dict[str, Any]:
    """Analyze code behavior patterns and side effects."""
    patterns = {
        'io_operations': [],
        'state_modifications': [],
        'async_operations': [],
        'error_handling': [],
        'resource_management': []
    }
    
    try:
        tree = ast.parse(entity.code)
        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    func_name = node.func.id
                    if func_name in ['open', 'read', 'write']:
                        patterns['io_operations'].append({
                            'type': func_name,
                            'line': node.lineno
                        })
                    elif func_name in ['append', 'extend', 'update']:
                        patterns['state_modifications'].append({
                            'type': func_name,
                            'line': node.lineno
                        })
            
            if isinstance(node, ast.Try):
                patterns['error_handling'].append({
                    'line': node.lineno,
                    'handlers': [h.type.id for h in node.handlers if isinstance(h.type, ast.Name)]
                })
            
            if isinstance(node, ast.AsyncFunctionDef):
                patterns['async_operations'].append({
                    'name': node.name,
                    'line': node.lineno
                })
    
    except Exception as e:
        logger.error(f"Error analyzing code behavior: {e}")
    
    return patterns

def calculate_complexity_metrics(entity: CodeEntity) -> Dict[str, Any]:
    """Calculate code complexity metrics."""
    metrics = {
        'cyclomatic_complexity': 0,
        'cognitive_complexity': 0,
        'halstead_metrics': {
            'volume': 0,
            'difficulty': 0,
            'effort': 0
        }
    }
    
    try:
        tree = ast.parse(entity.code)
        metrics['cyclomatic_complexity'] = calculate_cyclomatic_complexity(tree)
        metrics['cognitive_complexity'] = calculate_cognitive_complexity(tree)
        metrics['halstead_metrics'] = calculate_halstead_metrics(tree)
    except Exception as e:
        logger.error(f"Error calculating complexity metrics: {e}")
    
    return metrics

def convert_graph_to_visualization_format(graph: nx.DiGraph) -> Dict[str, Any]:
    """
    Convert NetworkX graph to a format suitable for visualization.
    
    Args:
        graph: NetworkX DiGraph object.
        
    Returns:
        Dictionary with nodes and edges in a visualization-friendly format.
    """
    # Extract nodes with their attributes
    nodes = []
    for node, attrs in graph.nodes(data=True):
        # Basic attributes all nodes should have
        node_data = {
            'id': node,
            'label': node,
            'type': attrs.get('type', 'unknown')
        }
        
        # Add other attributes if present
        if 'path' in attrs:
            node_data['path'] = attrs['path']
        
        if 'code_size' in attrs:
            node_data['code_size'] = attrs['code_size']
        
        nodes.append(node_data)
    
    # Extract edges with their attributes
    edges = []
    for source, target, attrs in graph.edges(data=True):
        edge_data = {
            'source': source,
            'target': target,
            'relationship': attrs.get('relationship', 'unknown')
        }
        edges.append(edge_data)
    
    return {
        'nodes': nodes,
        'edges': edges
    }

def extract_graph_metrics(graph: nx.DiGraph) -> Dict[str, Any]:
    """
    Extract metrics from the knowledge graph.
    
    Args:
        graph: NetworkX DiGraph object.
        
    Returns:
        Dictionary with graph metrics.
    """
    metrics = {
        'node_count': len(graph.nodes),
        'edge_count': len(graph.edges),
        'density': nx.density(graph),
        'connected': nx.is_weakly_connected(graph),
        'strongly_connected': nx.is_strongly_connected(graph),
        'centrality': {}
    }
    
    # Calculate centrality measures
    try:
        # Degree centrality
        in_degree = nx.in_degree_centrality(graph)
        out_degree = nx.out_degree_centrality(graph)
        
        # Betweenness centrality
        betweenness = nx.betweenness_centrality(graph)
        
        # Store centrality metrics for top nodes
        centrality_metrics = {
            'in_degree': dict(sorted(in_degree.items(), key=lambda x: x[1], reverse=True)[:10]),
            'out_degree': dict(sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:10]),
            'betweenness': dict(sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10])
        }
        
        metrics['centrality'] = centrality_metrics
    except Exception as e:
        logger.warning(f"Error calculating centrality metrics: {e}")
    
    # Calculate connected components
    try:
        # Weakly connected components
        weakly_connected = list(nx.weakly_connected_components(graph))
        metrics['weakly_connected_components'] = len(weakly_connected)
        
        # Strongly connected components
        strongly_connected = list(nx.strongly_connected_components(graph))
        metrics['strongly_connected_components'] = len(strongly_connected)
    except Exception as e:
        logger.warning(f"Error calculating connected components: {e}")
    
    return metrics

def detect_communities(graph: nx.DiGraph) -> List[Dict[str, Any]]:
    """
    Detect communities in the knowledge graph using community detection algorithms.
    
    Args:
        graph: NetworkX DiGraph object.
        
    Returns:
        List of community dictionaries with members and metrics.
    """
    communities = []
    
    try:
        # Convert directed graph to undirected for community detection
        undirected_graph = graph.to_undirected()
        
        # Use Girvan-Newman algorithm for community detection
        comp = nx.community.girvan_newman(undirected_graph)
        
        # Get the first level of communities
        node_groups = tuple(sorted(c) for c in next(comp))
        
        # Create community data
        for i, group in enumerate(node_groups):
            community = {
                'id': i,
                'members': list(group),
                'size': len(group),
                'types': {}
            }
            
            # Count node types in this community
            for node in group:
                if node in graph.nodes:
                    node_type = graph.nodes[node].get('type', 'unknown')
                    if node_type in community['types']:
                        community['types'][node_type] += 1
                    else:
                        community['types'][node_type] = 1
            
            communities.append(community)
    except Exception as e:
        logger.warning(f"Error detecting communities: {e}")
        
        # Fallback: create communities based on node types
        type_communities = {}
        
        for node, attrs in graph.nodes(data=True):
            node_type = attrs.get('type', 'unknown')
            
            if node_type not in type_communities:
                type_communities[node_type] = {
                    'id': len(type_communities),
                    'members': [],
                    'size': 0,
                    'types': {node_type: 0}
                }
            
            type_communities[node_type]['members'].append(node)
            type_communities[node_type]['size'] += 1
            type_communities[node_type]['types'][node_type] += 1
        
        communities = list(type_communities.values())
    
    return communities

def get_subgraph_for_entity(analysis: CodebaseAnalysis, entity_name: str, depth: int = 2) -> Dict[str, Any]:
    """
    Get a subgraph centered on a specific entity.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        entity_name: Name of the entity to center the subgraph on.
        depth: Depth of the neighborhood to include.
        
    Returns:
        Subgraph data in visualization format.
    """
    # Create the full graph first
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        node_attrs = {
            'type': entity.type,
            'path': entity.path,
            'code_size': len(entity.code.split('\n')),
            'metadata': entity.metadata
        }
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for src, deps in analysis.dependencies.items():
        for dest in deps:
            if dest in graph.nodes:
                graph.add_edge(src, dest, relationship='depends_on')
    
    # Check if the entity exists
    if entity_name not in graph.nodes:
        return {'nodes': [], 'edges': []}
    
    # Get k-hop neighborhood
    nodes_to_include = {entity_name}
    current_nodes = {entity_name}
    
    # Expand outward to get dependencies
    for _ in range(depth):
        next_nodes = set()
        for node in current_nodes:
            next_nodes.update(graph.successors(node))
        nodes_to_include.update(next_nodes)
        current_nodes = next_nodes
    
    # Reset current nodes to start over
    current_nodes = {entity_name}
    
    # Expand inward to get dependents
    for _ in range(depth):
        next_nodes = set()
        for node in current_nodes:
            next_nodes.update(graph.predecessors(node))
        nodes_to_include.update(next_nodes)
        current_nodes = next_nodes
    
    # Create subgraph
    subgraph = graph.subgraph(nodes_to_include)
    
    # Convert to visualization format
    return convert_graph_to_visualization_format(subgraph)

def get_entity_impact(analysis: CodebaseAnalysis, entity_name: str) -> Dict[str, Any]:
    """
    Calculate the impact of changing an entity on the rest of the codebase.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        entity_name: Name of the entity to analyze.
        
    Returns:
        Dictionary with impact analysis.
    """
    # Create the full graph first
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        node_attrs = {
            'type': entity.type,
            'path': entity.path
        }
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for src, deps in analysis.dependencies.items():
        for dest in deps:
            if dest in graph.nodes:
                graph.add_edge(src, dest, relationship='depends_on')
    
    # Check if the entity exists
    if entity_name not in graph.nodes:
        return {
            'entity': entity_name,
            'exists': False,
            'impact': 0,
            'affected_entities': []
        }
    
    # Get all entities that depend on this entity (directly or indirectly)
    try:
        # Get reverse reachable nodes (entities that depend on this one)
        affected_nodes = set()
        
        # BFS to find all nodes that depend on this entity
        queue = [entity_name]
        visited = set(queue)
        
        while queue:
            current = queue.pop(0)
            
            # Skip the original entity when counting affected nodes
            if current != entity_name:
                affected_nodes.add(current)
            
            # Add predecessors (nodes that depend on current)
            for pred in graph.predecessors(current):
                if pred not in visited:
                    visited.add(pred)
                    queue.append(pred)
        
        # Get details of affected entities
        affected_entities = []
        for node in affected_nodes:
            if node in graph.nodes:
                affected_entities.append({
                    'name': node,
                    'type': graph.nodes[node].get('type', 'unknown'),
                    'path': graph.nodes[node].get('path', 'unknown')
                })
        
        # Calculate impact score (percentage of codebase affected)
        impact_score = len(affected_nodes) / (len(graph.nodes) - 1) if len(graph.nodes) > 1 else 0
        
        return {
            'entity': entity_name,
            'exists': True,
            'impact': impact_score,
            'affected_count': len(affected_nodes),
            'affected_entities': affected_entities
        }
    
    except Exception as e:
        logger.error(f"Error calculating entity impact: {e}")
        return {
            'entity': entity_name,
            'exists': True,
            'impact': 0,
            'error': str(e),
            'affected_entities': []
        }

def find_core_components(analysis: CodebaseAnalysis) -> List[Dict[str, Any]]:
    """
    Identify core components of the codebase based on centrality measures.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        
    Returns:
        List of core components with their metrics.
    """
    # Create graph
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        node_attrs = {
            'type': entity.type,
            'path': entity.path
        }
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for src, deps in analysis.dependencies.items():
        for dest in deps:
            if dest in graph.nodes:
                graph.add_edge(src, dest)
    
    try:
        # Calculate various centrality measures
        betweenness = nx.betweenness_centrality(graph)
        in_degree = nx.in_degree_centrality(graph)
        out_degree = nx.out_degree_centrality(graph)
        
        # Combine the measures for ranking
        core_scores = {}
        for node in graph.nodes:
            # Weighted combination of centrality measures
            core_scores[node] = (
                betweenness.get(node, 0) * 0.5 +
                in_degree.get(node, 0) * 0.3 +
                out_degree.get(node, 0) * 0.2
            )
        
        # Sort by score
        sorted_components = sorted(core_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Create result with top components (up to 10)
        core_components = []
        for name, score in sorted_components[:10]:
            if score > 0:
                component = {
                    'name': name,
                    'type': graph.nodes[name].get('type', 'unknown'),
                    'path': graph.nodes[name].get('path', 'unknown'),
                    'core_score': score,
                    'betweenness': betweenness.get(name, 0),
                    'in_degree': in_degree.get(name, 0),
                    'out_degree': out_degree.get(name, 0)
                }
                core_components.append(component)
        
        return core_components
    
    except Exception as e:
        logger.error(f"Error identifying core components: {e}")
        return []
