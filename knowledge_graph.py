"""
File: knowledge_graph.py
Purpose: Create knowledge graph from code analysis
Generated by Code Reverse-Engineering System
"""

import logging
import networkx as nx
import json
from typing import Dict, List, Any, Optional, Tuple
import os
from models import CodebaseAnalysis, CodeEntity

# Setup logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def create_knowledge_graph(analysis: CodebaseAnalysis) -> Dict[str, Any]:
    """
    Create a knowledge graph from codebase analysis results.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        
    Returns:
        Dictionary with knowledge graph data, compatible with visualization tools.
    """
    logger.info("Creating knowledge graph from codebase analysis")
    
    # Create a directed graph
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        # Node attributes
        node_attrs = {
            'type': entity.type,
            'path': entity.path,
            'code_size': len(entity.code.split('\n')),
            'metadata': entity.metadata
        }
        
        # Add node
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for entity_name, dependencies in analysis.dependencies.items():
        for dependency in dependencies:
            # Check if dependency exists in the graph
            if dependency in graph.nodes:
                # Add edge
                graph.add_edge(entity_name, dependency, relationship='depends_on')
    
    # Add imports/exports relationships
    for entity in analysis.entities:
        if entity.type == 'module':
            # Check for imports
            if 'imports' in entity.metadata:
                for imported in entity.metadata['imports']:
                    # Find matching entity by name
                    imported_name = imported.split('.')[-1]  # Extract the module name
                    if imported_name in graph.nodes:
                        graph.add_edge(entity.name, imported_name, relationship='imports')
    
    # Add class inheritance relationships
    for entity in analysis.entities:
        if entity.type == 'class' and 'parent_classes' in entity.metadata:
            for parent in entity.metadata['parent_classes']:
                if parent in graph.nodes:
                    graph.add_edge(entity.name, parent, relationship='inherits_from')
    
    # Add file relationships
    file_entities = {}
    for entity in analysis.entities:
        file_path = entity.path
        if file_path not in file_entities:
            file_entities[file_path] = []
        file_entities[file_path].append(entity.name)
    
    # Add file nodes and edges to contained entities
    for file_path, entities in file_entities.items():
        file_name = os.path.basename(file_path)
        
        # Add file node
        graph.add_node(file_name, type='file', path=file_path)
        
        # Connect file to its entities
        for entity_name in entities:
            graph.add_edge(file_name, entity_name, relationship='contains')
    
    # Convert to serializable format for visualization
    graph_data = convert_graph_to_visualization_format(graph)
    
    # Extract additional graph metrics
    graph_metrics = extract_graph_metrics(graph)
    
    # Create community detection
    communities = detect_communities(graph)
    
    # Return the complete knowledge graph data
    knowledge_graph = {
        'graph': graph_data,
        'metrics': graph_metrics,
        'communities': communities
    }
    
    logger.info(f"Created knowledge graph with {len(graph.nodes)} nodes and {len(graph.edges)} edges")
    
    return knowledge_graph

def convert_graph_to_visualization_format(graph: nx.DiGraph) -> Dict[str, Any]:
    """
    Convert NetworkX graph to a format suitable for visualization.
    
    Args:
        graph: NetworkX DiGraph object.
        
    Returns:
        Dictionary with nodes and edges in a visualization-friendly format.
    """
    # Extract nodes with their attributes
    nodes = []
    for node, attrs in graph.nodes(data=True):
        # Basic attributes all nodes should have
        node_data = {
            'id': node,
            'label': node,
            'type': attrs.get('type', 'unknown')
        }
        
        # Add other attributes if present
        if 'path' in attrs:
            node_data['path'] = attrs['path']
        
        if 'code_size' in attrs:
            node_data['code_size'] = attrs['code_size']
        
        nodes.append(node_data)
    
    # Extract edges with their attributes
    edges = []
    for source, target, attrs in graph.edges(data=True):
        edge_data = {
            'source': source,
            'target': target,
            'relationship': attrs.get('relationship', 'unknown')
        }
        edges.append(edge_data)
    
    return {
        'nodes': nodes,
        'edges': edges
    }

def extract_graph_metrics(graph: nx.DiGraph) -> Dict[str, Any]:
    """
    Extract metrics from the knowledge graph.
    
    Args:
        graph: NetworkX DiGraph object.
        
    Returns:
        Dictionary with graph metrics.
    """
    metrics = {
        'node_count': len(graph.nodes),
        'edge_count': len(graph.edges),
        'density': nx.density(graph),
        'connected': nx.is_weakly_connected(graph),
        'strongly_connected': nx.is_strongly_connected(graph),
        'centrality': {}
    }
    
    # Calculate centrality measures
    try:
        # Degree centrality
        in_degree = nx.in_degree_centrality(graph)
        out_degree = nx.out_degree_centrality(graph)
        
        # Betweenness centrality
        betweenness = nx.betweenness_centrality(graph)
        
        # Store centrality metrics for top nodes
        centrality_metrics = {
            'in_degree': dict(sorted(in_degree.items(), key=lambda x: x[1], reverse=True)[:10]),
            'out_degree': dict(sorted(out_degree.items(), key=lambda x: x[1], reverse=True)[:10]),
            'betweenness': dict(sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10])
        }
        
        metrics['centrality'] = centrality_metrics
    except Exception as e:
        logger.warning(f"Error calculating centrality metrics: {e}")
    
    # Calculate connected components
    try:
        # Weakly connected components
        weakly_connected = list(nx.weakly_connected_components(graph))
        metrics['weakly_connected_components'] = len(weakly_connected)
        
        # Strongly connected components
        strongly_connected = list(nx.strongly_connected_components(graph))
        metrics['strongly_connected_components'] = len(strongly_connected)
    except Exception as e:
        logger.warning(f"Error calculating connected components: {e}")
    
    return metrics

def detect_communities(graph: nx.DiGraph) -> List[Dict[str, Any]]:
    """
    Detect communities in the knowledge graph using community detection algorithms.
    
    Args:
        graph: NetworkX DiGraph object.
        
    Returns:
        List of community dictionaries with members and metrics.
    """
    communities = []
    
    try:
        # Convert directed graph to undirected for community detection
        undirected_graph = graph.to_undirected()
        
        # Use Girvan-Newman algorithm for community detection
        comp = nx.community.girvan_newman(undirected_graph)
        
        # Get the first level of communities
        node_groups = tuple(sorted(c) for c in next(comp))
        
        # Create community data
        for i, group in enumerate(node_groups):
            community = {
                'id': i,
                'members': list(group),
                'size': len(group),
                'types': {}
            }
            
            # Count node types in this community
            for node in group:
                if node in graph.nodes:
                    node_type = graph.nodes[node].get('type', 'unknown')
                    if node_type in community['types']:
                        community['types'][node_type] += 1
                    else:
                        community['types'][node_type] = 1
            
            communities.append(community)
    except Exception as e:
        logger.warning(f"Error detecting communities: {e}")
        
        # Fallback: create communities based on node types
        type_communities = {}
        
        for node, attrs in graph.nodes(data=True):
            node_type = attrs.get('type', 'unknown')
            
            if node_type not in type_communities:
                type_communities[node_type] = {
                    'id': len(type_communities),
                    'members': [],
                    'size': 0,
                    'types': {node_type: 0}
                }
            
            type_communities[node_type]['members'].append(node)
            type_communities[node_type]['size'] += 1
            type_communities[node_type]['types'][node_type] += 1
        
        communities = list(type_communities.values())
    
    return communities

def get_subgraph_for_entity(analysis: CodebaseAnalysis, entity_name: str, depth: int = 2) -> Dict[str, Any]:
    """
    Get a subgraph centered on a specific entity.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        entity_name: Name of the entity to center the subgraph on.
        depth: Depth of the neighborhood to include.
        
    Returns:
        Subgraph data in visualization format.
    """
    # Create the full graph first
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        node_attrs = {
            'type': entity.type,
            'path': entity.path,
            'code_size': len(entity.code.split('\n')),
            'metadata': entity.metadata
        }
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for src, deps in analysis.dependencies.items():
        for dest in deps:
            if dest in graph.nodes:
                graph.add_edge(src, dest, relationship='depends_on')
    
    # Check if the entity exists
    if entity_name not in graph.nodes:
        return {'nodes': [], 'edges': []}
    
    # Get k-hop neighborhood
    nodes_to_include = {entity_name}
    current_nodes = {entity_name}
    
    # Expand outward to get dependencies
    for _ in range(depth):
        next_nodes = set()
        for node in current_nodes:
            next_nodes.update(graph.successors(node))
        nodes_to_include.update(next_nodes)
        current_nodes = next_nodes
    
    # Reset current nodes to start over
    current_nodes = {entity_name}
    
    # Expand inward to get dependents
    for _ in range(depth):
        next_nodes = set()
        for node in current_nodes:
            next_nodes.update(graph.predecessors(node))
        nodes_to_include.update(next_nodes)
        current_nodes = next_nodes
    
    # Create subgraph
    subgraph = graph.subgraph(nodes_to_include)
    
    # Convert to visualization format
    return convert_graph_to_visualization_format(subgraph)

def get_entity_impact(analysis: CodebaseAnalysis, entity_name: str) -> Dict[str, Any]:
    """
    Calculate the impact of changing an entity on the rest of the codebase.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        entity_name: Name of the entity to analyze.
        
    Returns:
        Dictionary with impact analysis.
    """
    # Create the full graph first
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        node_attrs = {
            'type': entity.type,
            'path': entity.path
        }
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for src, deps in analysis.dependencies.items():
        for dest in deps:
            if dest in graph.nodes:
                graph.add_edge(src, dest, relationship='depends_on')
    
    # Check if the entity exists
    if entity_name not in graph.nodes:
        return {
            'entity': entity_name,
            'exists': False,
            'impact': 0,
            'affected_entities': []
        }
    
    # Get all entities that depend on this entity (directly or indirectly)
    try:
        # Get reverse reachable nodes (entities that depend on this one)
        affected_nodes = set()
        
        # BFS to find all nodes that depend on this entity
        queue = [entity_name]
        visited = set(queue)
        
        while queue:
            current = queue.pop(0)
            
            # Skip the original entity when counting affected nodes
            if current != entity_name:
                affected_nodes.add(current)
            
            # Add predecessors (nodes that depend on current)
            for pred in graph.predecessors(current):
                if pred not in visited:
                    visited.add(pred)
                    queue.append(pred)
        
        # Get details of affected entities
        affected_entities = []
        for node in affected_nodes:
            if node in graph.nodes:
                affected_entities.append({
                    'name': node,
                    'type': graph.nodes[node].get('type', 'unknown'),
                    'path': graph.nodes[node].get('path', 'unknown')
                })
        
        # Calculate impact score (percentage of codebase affected)
        impact_score = len(affected_nodes) / (len(graph.nodes) - 1) if len(graph.nodes) > 1 else 0
        
        return {
            'entity': entity_name,
            'exists': True,
            'impact': impact_score,
            'affected_count': len(affected_nodes),
            'affected_entities': affected_entities
        }
    
    except Exception as e:
        logger.error(f"Error calculating entity impact: {e}")
        return {
            'entity': entity_name,
            'exists': True,
            'impact': 0,
            'error': str(e),
            'affected_entities': []
        }

def find_core_components(analysis: CodebaseAnalysis) -> List[Dict[str, Any]]:
    """
    Identify core components of the codebase based on centrality measures.
    
    Args:
        analysis: CodebaseAnalysis object with analysis results.
        
    Returns:
        List of core components with their metrics.
    """
    # Create graph
    graph = nx.DiGraph()
    
    # Add entities as nodes
    for entity in analysis.entities:
        node_attrs = {
            'type': entity.type,
            'path': entity.path
        }
        graph.add_node(entity.name, **node_attrs)
    
    # Add dependencies as edges
    for src, deps in analysis.dependencies.items():
        for dest in deps:
            if dest in graph.nodes:
                graph.add_edge(src, dest)
    
    try:
        # Calculate various centrality measures
        betweenness = nx.betweenness_centrality(graph)
        in_degree = nx.in_degree_centrality(graph)
        out_degree = nx.out_degree_centrality(graph)
        
        # Combine the measures for ranking
        core_scores = {}
        for node in graph.nodes:
            # Weighted combination of centrality measures
            core_scores[node] = (
                betweenness.get(node, 0) * 0.5 +
                in_degree.get(node, 0) * 0.3 +
                out_degree.get(node, 0) * 0.2
            )
        
        # Sort by score
        sorted_components = sorted(core_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Create result with top components (up to 10)
        core_components = []
        for name, score in sorted_components[:10]:
            if score > 0:
                component = {
                    'name': name,
                    'type': graph.nodes[name].get('type', 'unknown'),
                    'path': graph.nodes[name].get('path', 'unknown'),
                    'core_score': score,
                    'betweenness': betweenness.get(name, 0),
                    'in_degree': in_degree.get(name, 0),
                    'out_degree': out_degree.get(name, 0)
                }
                core_components.append(component)
        
        return core_components
    
    except Exception as e:
        logger.error(f"Error identifying core components: {e}")
        return []
